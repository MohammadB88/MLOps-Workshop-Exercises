{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the MLOps Workshop!","text":"<p>MLOps (Machine Learning Operations) is a discipline that combines machine learning, software engineering, and DevOps practices to streamline the development, deployment, and maintenance of machine learning models in production. It focuses on automating and monitoring the entire ML lifecycle, from data preparation and model training to version control, testing, and scalable deployment. By fostering collaboration between data scientists, engineers, and IT teams, MLOps ensures models are reliable, reproducible, and capable of delivering business value in real-world environments.</p> <p> </p> <p>You will learn all about MLOps tools and Principals in 5 steps:</p> <ol> <li> <p>Provide foundational knowledge about MLOps and highlight common challenges in operationalizing machine learning models.</p> <ul> <li>Define MLOps, its principles, and how it integrates machine learning, DevOps, and software engineering.</li> <li>Discuss challenges such as reproducibility, scalability, collaboration, and monitoring.</li> <li>Explore the ML lifecycle and its pain points in production environments (e.g., data drift, model degradation).</li> </ul> </li> <li> <p>Train a machine learning model as a baseline for deployment.</p> <ul> <li>Load and preprocess a sample dataset (e.g., sklearn\u2019s Iris or custom tabular data).</li> <li>Train a model using a Python ML framework like Scikit-learn or TensorFlow.   </li> </ul> </li> <li> <p>Demonstrate how to use MLflow for experiment tracking and registering models.</p> <ul> <li>Set up MLflow tracking server locally or in the cloud.</li> <li>Log experiments: track metrics, parameters, and artifacts using MLflow's API.</li> <li>Register the best-performing model in MLflow\u2019s model registry and add metadata (version, tags, etc.).</li> <li>Briefly explain the benefits of a centralized model registry for collaboration and version control.</li> </ul> </li> <li> <p>Deploy the registered model to a production-like cluster environment.</p> <ul> <li>Package the model as a REST API using tools like Flask or FastAPI.</li> <li>Deploy the API to a cluster environment (e.g., Kubernetes)    </li> </ul> </li> <li> <p>Monitor the deployed model in production using Evidently to detect issues like data drift or performance degradation.</p> <ul> <li>Set up Evidently dashboards to track key metrics, including feature distribution, target drift, and prediction quality.</li> <li>Simulate data drift by sending new datasets to the model\u2019s API and analyze the impact using Evidently.   </li> </ul> </li> </ol> <p>After finishing this lab you are ready to start using MLOps tools and principals to deploy your models in production.</p>"},{"location":"#time-planning","title":"Time planning","text":"<p>TODO The time required to do the workshops strongly depends on multiple factors: the length of the workshop. the number of participants, how familiar you are with Linux, Containers, and Kubernetes in general as well as topics like Data Engineering and Machine Learning.</p>"},{"location":"#lab-overview","title":"Lab Overview","text":"<p>TODO</p>"},{"location":"#from-other-sources","title":"From other sources:","text":"<p>Machine learning models require extra consideration for monitoring models in production and retraining if the performance declines. </p> <p>The engineering practice of MLOps leverages three contributing disciplines: machine learning, software  engineering (especially DevOps), and data engineering. The goal of MLOps is to bridge the gap between  development (Dev) and operations (Ops) and create a repeatable process for training, deploying,  monitoring, and updating machine learning models. </p> <p>MLOps provides tools and processes so that collaborators can contribute their part to achieve a seamless flow from model request to deploying models to solve business problems. </p> <p>Key components of the MLOps pipeline are:  \u2022 Data management  \u2022 Model development  \u2022 Model deployment  \u2022 Monitoring and optimization  \u2022 Collaboration and governance</p> <p>HOW A USECASE FOR A MLOPS WORKSHOP SHOULD LOOKE LIKE:</p> <p>An insurance company has data on their customers that includes customer information, policies, and  claims. The policies include a risk factor calculated using an age-old formula. They want to know if they  can improve on it by adding information such as accidents in the Chicago area. The goal is not necessarily to provide a new model to replace the old model but to show how IBM watsonx  platform can facilitate MLOps. The project covers five parts. 1. Preparing data: exploring the data, adding new data, preparing the data for training  2. Developing the model: developing the model via AutoAI, Jupyter notebook, SPSS flow  3. Deploying the model: via UI and Jupyter notebooks  4. Monitoring the model via OpenScale and AI Factsheets  5. Automating the MLOps lifecycle </p>"},{"location":"git-cheatsheet/","title":"Git CheatSheet","text":"<p>This chapter lists the most important and most frequently used Git commands. These commands can be used to interact with the Git repositories mentioned in the use cases.</p>"},{"location":"git-cheatsheet/#basic-git-commands","title":"Basic Git Commands","text":"Command Description <code>git clone &lt;repo&gt;</code> Clone an existing repository <code>git status</code> Show the working directory status <code>git add &lt;file&gt;</code> Add a file to the staging area <code>git add .</code> Add all files to the staging area <code>git commit -m \"message\"</code> Commit changes with a message <code>git push</code> Push changes to the remote repository <code>git pull</code> Fetch and merge changes from the remote repository <code>git branch</code> List all branches <code>git branch &lt;name&gt;</code> Create a new branch <code>git checkout &lt;branch&gt;</code> Switch to a branch <code>git checkout -b &lt;branch&gt;</code> Create and switch to a new branch <code>git log</code> Show commit logs <p>Note</p> <p>This cheatsheet is not intended to be exhaustive, but merely serves as a reminder of the most important commands.</p>"},{"location":"mlflow-installation/","title":"MLflow Installation with helm","text":"<p>In order to install mlflow on OpenShift using helm charts from bitnami, following steps are necessary:</p> <ol> <li> <p>Open a terminal on the bastian host or direct connect to a <code>\"web terminal\"</code> client on the openshift.</p> </li> <li> <p>Create a namespace/project <code>`mlflow</code> in the cluster: <pre><code>oc new-project mlflow \n</code></pre></p> </li> <li> <p>Add the bitnami charts as a repository: <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\n</code></pre></p> </li> </ol> <p>you can check if the repository is correctly added: <pre><code>helm repo list\nhelm rpeo update\n</code></pre></p> <ol> <li> <p>Copy the values of the mlflow charts in a yaml file: <pre><code>helm show values bitnami/mlflow &gt; mlflow_values.yaml\n</code></pre></p> </li> <li> <p>Open the <code>mlflow_values.yaml</code> with editor (usually vim/vi): <pre><code>vim mlflow_values.yaml\n</code></pre></p> </li> </ol> <p>edit two parts:</p> <p>The service type for tracking server is is <code>LoadBalancer</code>, which should be ClusterIP (Search for <code>\"LoadBalancer\"</code> in the values file you just generated): <pre><code>LoadBalancer -&gt; ClusterIP\n</code></pre></p> <p>We will later add a route to access the UI of the mlflow tracking server.</p> <p>Disable the authentification for tracking server (Search for <code>\"username: user\"</code> in the values file you just generated): <pre><code>enabled: true -&gt; enabled: false\nusername: user\n</code></pre></p> <ol> <li>Install the MLflow using helm charts and these values: <pre><code>helm install mlflow bitnami/mlflow --namespace mlflow --values mlflow_values.yaml\n</code></pre></li> </ol>"},{"location":"mlops-overview/","title":"Overview","text":"<p>TODO</p> <p>What is MLOps and why and which tools and workflows.</p>"},{"location":"workshop-overview/","title":"Overview","text":"<p>TODO</p> <p>In this page we will go thorough a simple example of implementing MLOps concept to the Wine Quality Classifier.</p> <p>You will complete the following exercises today:</p> <ul> <li>Wine Quality Classifier</li> <li>Bike Demand Forecasting</li> </ul>"},{"location":"demo-environment/red-hat-demo-environment/","title":"RedHat Demo Environment","text":"<p>Every Workshop attendant has his own demo environment.</p>"},{"location":"demo-environment/red-hat-demo-environment/#lab-diagram","title":"Lab Diagram","text":"<p>TODO</p> <p>Here the configuration and architecture the demo environment will be explained.</p>"},{"location":"mlops/bike-demand-forecasting/","title":"Forecasting Bike-Sharing Demand and Monitoring Model Performance","text":""},{"location":"mlops/bike-demand-forecasting/#introduction","title":"\ud83d\udeb2 Introduction","text":"<p>In urban environments, bike-sharing systems have emerged as a sustainable and efficient mode of transportation. Accurately predicting bike rental demand is crucial for optimizing operations, ensuring bike availability, and enhancing user satisfaction.</p> <p>This documentation presents a comprehensive approach to developing and deploying a machine learning model for bike-sharing demand forcasting. Drawing inspiration from Analytics Vidhya's end-to-end case study, we delve into data preprocessing, feature engineering, model training, and evaluation. </p> <p>Beyond model development, maintaining performance in a production environment is vital. Models can degrade over time due to changing data patterns, a phenomenon known as concept drift. To address this, we incorporate monitoring strategies inspired by Evidently AI's tutorial on production model analytics. This includes setting up regular performance checks and generating interactive reports to detect issues proactively.</p> <p>By following this guide, you'll gain insights into building a robust machine learning pipeline for bike-sharing demand prediction and implementing effective monitoring to ensure sustained model performance in real-world applications.</p>"},{"location":"mlops/bike-demand-forecasting/#overview-of-the-problem","title":"Overview of the Problem","text":"<p>A company operates a bike-sharing platform and wants to understand and predict bike demand on an hourly basis. To support this, a dataset has been provided containing hourly records enriched with contextual features such as weather, season, and the day of the week.</p> <p>The goal is to analyze this dataset and develop a machine learning model that can forecast hourly demand for bikes. This problem simulates a real-world scenario where the company only has access to historical data from January and February. Using this limited data, we train a Random Forest Regressor to predict future demand.</p> <p>This setup reflects several practical challenges faced in deploying and maintaining models in production:</p> <ul> <li> <p>Data availability and delay: Usage data may be stored locally and only synced with a central database weekly or even monthly, making real-time monitoring and updates difficult.</p> </li> <li> <p>Gradual model decay: As time progresses, patterns in user behavior or external conditions (e.g., weather, seasonality) may change. Since the model is trained only on winter data (January and February), its performance may gradually deteriorate as new, unseen seasonal patterns emerge. This phenomenon\u2014where a model's predictive quality declines over time\u2014is known as gradual model decay.</p> </li> </ul> <p>Understanding and mitigating this decay is crucial for maintaining reliable predictions in production.</p>"},{"location":"mlops/bike-demand-forecasting/#data-and-considered-featurers","title":"Data and Considered Featurers","text":"<p>We see the considered features in the below image (How to break a model in 20 days!):</p> <p> </p> <p>Here is a list of features that we condsider when training the model:</p> <ul> <li>hour - hourly  </li> <li>weekday - day of the week</li> <li>weather situation (weathersit) - <ul> <li>1: Clear, Few clouds, Partly cloudy, Partly cloudy</li> <li>2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist</li> <li>3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds</li> <li>4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog </li> </ul> </li> <li>temp - temperature in Celsius </li> <li>atemp - \"feels like\" temperature in Celsius</li> <li>humidity - relative humidity</li> <li>windspeed - wind speed</li> <li>season -  <ul> <li>1 = spring</li> <li>2 = summer</li> <li>3 = fall</li> <li>4 = winter </li> </ul> </li> <li>holiday - whether the day is a holiday</li> <li>workingday - whether the day is a working day (i.e., not a weekend or holiday)</li> <li>count - total number of rentals (target variable)</li> </ul>"},{"location":"mlops/bike-demand-forecasting/#model-random-forest-regressor","title":"Model: Random Forest Regressor","text":"<p>The Random Forest Regressor is an ensemble machine learning model that builds multiple decision trees and averages their predictions to improve accuracy and reduce overfitting. Each tree is trained on a random subset of the data and features, which introduces diversity and helps generalize better to unseen data. By aggregating the outputs of many weak learners (trees), the model provides robust and stable regression predictions.</p> <p>It offers a variety of hyperparameters that one can tune to adjust and potentially improve the model\u2019s performance.  For this example, we only play with two of these hyperparameters:</p> <ul> <li> <p><code>n_estimators</code>:  Number of decision trees that the model builds (i.e 50, 100, 150, 200)</p> </li> <li> <p><code>max_depth</code>: Maximum depth of each tree (i.e 5, 10, 15, 20)</p> </li> </ul>"},{"location":"mlops/bike-demand-forecasting/#directory-structure","title":"Directory Structure","text":"<p>The bike_demand_forecasting project is organized into modular folders for clarity and workflow. It includes raw and processed datasets under data/, Jupyter notebooks for each MLOps stage in notebooks/, trained models in models/, and generated analysis or drift reports in reports/\u2014supporting a full ML lifecycle.</p> <pre><code>bike_demand_forecasting/\n\u251c\u2500\u2500 01_data_exploration.ipynb\n\u251c\u2500\u2500 02_model_training.ipynb\n\u251c\u2500\u2500 03_model_deployment.ipynb\n\u251c\u2500\u2500 04_model_monitoring.ipynb\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 raw/\n\u2502   \u251c\u2500\u2500 processed/\n\u2502   \u2514\u2500\u2500 test_model/\n\u251c\u2500\u2500 models/\n\u2514\u2500\u2500 reports/\nbike_demand_forecasting_pipeline/\n\u2514\u2500\u2500 pipeline_bike_sharing.py\n</code></pre>"},{"location":"mlops/bike-demand-forecasting/#prerequisites","title":"Prerequisites","text":""},{"location":"mlops/bike-demand-forecasting/#environment","title":"Environment","text":"<p>You can use your own JupyterLab environment.</p> <p>Otherwise, use the provided environment (e.g. OpenShift AI).  Log in to this environment with username and password provided during the workshop.</p>"},{"location":"mlops/bike-demand-forecasting/#hands-on-sessions","title":"\ud83d\udcd8 Hands-On Sessions","text":"<p>To build a strong foundation in MLOps, participants will begin by executing each stage of the machine learning workflow manually. This hands-on approach helps solidify the concepts and understand how data and models progress through the pipeline.</p> <p>\ud83d\udca1 Note One: The link to the Training Environment and Credentials will be provided at the begenning of the workshop!</p> <p>\ud83d\udca1 Note Two: Workshop materials for tasks 1 to 7 are stored under the path: <code>\"workshop_materials/bike_demand_forecasting\"</code></p> <p>\ud83d\udca1 Note Three: Workshop materials for tasks 8 are stored under the path: <code>\"workshop_materials/bike_demand_forecasting_pipeline\"</code></p>"},{"location":"mlops/bike-demand-forecasting/#task-1-clone-the-repository-load-and-explore-the-data","title":"Task 1 - Clone the repository &amp; Load and explore the data","text":""},{"location":"mlops/bike-demand-forecasting/#task-2-data-preparations-for-model-training","title":"Task 2 - Data Preparations for Model Training","text":""},{"location":"mlops/bike-demand-forecasting/#task-3-model-training-experiment-tracking","title":"Task 3 - Model Training &amp; Experiment Tracking","text":""},{"location":"mlops/bike-demand-forecasting/#task-4-review-the-experiments-select-the-best-model","title":"Task 4 - Review the Experiments &amp; Select the Best Model","text":""},{"location":"mlops/bike-demand-forecasting/#task-5-model-deploymet-containerize-the-endpoint-api","title":"Task 5 - Model Deploymet - Containerize the Endpoint-API","text":""},{"location":"mlops/bike-demand-forecasting/#task-6-model-deploymet-deploy-on-openshift-cluster","title":"Task 6 - Model Deploymet - Deploy on OpenShift Cluster","text":""},{"location":"mlops/bike-demand-forecasting/#task-7-model-data-monitoring","title":"Task 7 - Model &amp; Data Monitoring","text":""},{"location":"mlops/bike-demand-forecasting/#task-8-automating-the-workflow-with-kubeflow-pipelines","title":"Task 8 - Automating the Workflow with Kubeflow Pipelines","text":""},{"location":"mlops/bike-demand-forecasting/#task-9-automating-the-workflow-with-elyra-pipelines","title":"Task 9 - Automating the Workflow with Elyra Pipelines","text":""},{"location":"mlops/wine-quality-classifier/","title":"Wine Quality Classifier","text":""},{"location":"mlops/wine-quality-classifier/#objective","title":"Objective","text":"<p>In this exercise there are 4 steps from data to deployed model onto the openshift cluster. For each of these steps, a Jupyter Notebook is prepared.</p>"},{"location":"mlops/wine-quality-classifier/#1-data-explorations-and-processing","title":"1. Data Explorations and Processing","text":"<p>Inside the first notebook we read the input data and perform some data exploration tasks. After processing and cleaning the data, it will be stored in a processed dataset. </p> <p>Processed data will be then splitted into 3 datasets, train_dataset, test_dataset, and perfomance_dataset and stored in the corresponding directories. </p>"},{"location":"mlops/wine-quality-classifier/#2-model-training-and-registration-as-well-as-experment-tracking","title":"2. Model Training and registration as well as Experment tracking","text":"<p>We take the preprocessed data and start training a model. The trained model as well as its performance and artifacts can be tracked using MLflow. </p> <p>Usually, data scientists will not be satisfied with the first try. Hence, they will experiment with various sets of hyperparameters and different models. Next, these experiments will be compared on MLflow UI and the model with the best performance will be selected. </p> <p>curl -X GET 'https://{us-south.ml.cloud.ibm.com}/ml/v1/foundation_model_specs?version=2024-07-25&amp;filters=function_embedding'</p>"},{"location":"mlops/wine-quality-classifier/#3-local-model-depoloyment-with-the-help-of-mlflow","title":"3. Local model depoloyment with the help of MLflow","text":"<p>This model can be deployed locally using  <pre><code>mlflow models serve -m \"models:/MODEL_NAME/MODEL_VERSION\" --env-manager local --no-conda\n</code></pre></p> <p>But before starting the mlflow model server, the varialbe MLFLOW_TRACKING_URI should be defined: <pre><code>export MLFLOW_TRACKING_URI=http://mlflow-mlflow.apps.cluster-db46l.dynamic.redhatworkshops.io\n</code></pre></p>"},{"location":"mlops/wine-quality-classifier/#4-model-packaging-and-deplyoment-on-the-openshift-cluster","title":"4. Model packaging and deplyoment on the Openshift cluster","text":"<p>Now that the model and its dependecies are pushed to the Github repository, we are ready to go to the openshift cluster and start building a container image for the model and deploying it on the cluster. finally we use the deployed model API to send prediction requests using a frontend application.</p> <p> </p> <p>There are two important hints:</p> <ul> <li>Take care of the specific permissions for images deployed on Openshift cluster.</li> </ul> <p>This line  <pre><code>RUN chgrp -R 0 /opt &amp;&amp; chmod -R g=u /opt\n</code></pre> should be added to the Dockerfile After below line: <pre><code>RUN chmod o+rwX /opt/mlflow/\n</code></pre></p> <ul> <li>Configuring BuildConfig from Formular is prone to error. So use the below template and adjust the corresponding variables. BuildConfig<pre><code>apiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  namespace: BUILD_CONFIG_NAMESPACE\n  name: BUILD_CONFIG_NAME\nspec:\n  source:\n    type: Git\n    git:\n      uri: 'https://GIT_REPO_URL'\n    contextDir: /GENERATE_DIRECTORY_CONTAINER_FILE\n  strategy:\n    type: Docker\n    dockerStrategy:\n      from:\n        kind: DockerImage\n        name: 'python:3.9.4-slim'\n  output:\n    to:\n      kind: ImageStreamTag\n      name: model-clf:1.0\n</code></pre></li> </ul> <p>As shown in the image, we should create an ImageStream resource and then a BuildConfig to configure the image building process.</p> <p>When the container is running and the model API is ready, we can send a test requests as follows:</p> test model API<pre><code># Send a prediction request to the depoloyed model on the OpenShift Cluster\nBASE_URL = \"http://model-clf-model-clf.apps.cluster-db46l.dynamic.redhatworkshops.io/\"\nendpoint = f\"{BASE_URL}/invocations\"\nresponse = requests.post(endpoint, json=inference_request)\n\n# Check if the response is successful\nif response.status_code == 200:\n    # Process the prediction response\n    predictions = pd.DataFrame(response.json()['predictions'], columns=['Predicted Wine Quality'])\n\n    # Combine predictions with actual classes\n    actual_class_test = test_df_5['best quality'].reset_index(drop=True)\n    model_output = pd.concat([predictions, actual_class_test], axis=1)\n\n    # Rename columns for clarity\n    model_output.columns = ['Predicted Wine Quality', 'Actual Wine Quality']\n\n    # Display the final output\n    print(model_output)\n\nelse:\n    print(f\"Request failed with status code: {response.status_code}\")\n    print(f\"Response content: {response.text}\")\n</code></pre>"},{"location":"mlops/wine-quality-classifier/#5-deploy-the-simple-webapp","title":"5. Deploy the simple webapp","text":"<p>In order to consume the deployed model we will integrate it with a simple webapp.  The code and Dockerfile for the app is in the same repository but in directory app_wine_Clf. </p> <p>We deploy the app in its own project. Let's call it app-wine-clf. Then create an imagestream and a buildconfig. </p> <p> </p> <p>After the image is built, we can deploy the app. In the deployment definition we can set the model API by setting the variable Endpoint_URL. </p> <p>When the app is deployed and reachable through its route, we can test the model again but this time from a nice interface.</p>"},{"location":"mlops/wine-quality-classifier/#6-model-and-data-monitoring","title":"6. Model and Data Monitoring","text":""},{"location":"mlops/tasks/task1/","title":"Task 1: Clone the Repository &amp; Load and Explore the Data","text":"<p>In this task, we will clone the repository containing the workshop materials and explore the dataset for bike sharing company.</p>"},{"location":"mlops/tasks/task1/#1-fork-the-repository","title":"1. Fork the Repository","text":"<ul> <li>Go to the original GitHub repository page.</li> <li>Click the \"Fork\" button in the top-right corner to create your own copy of the repo.</li> </ul>"},{"location":"mlops/tasks/task1/#2-generate-an-ssh-key","title":"2. Generate an SSH Key","text":"<p>Run the following command in your terminal:</p> <pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre>"},{"location":"mlops/tasks/task1/#3-copy-your-public-ssh-key","title":"3. Copy Your Public SSH Key","text":"<p>Copy the entire output (starts with ssh-ed25519) of this command. <pre><code>cat ~/.ssh/id_ed25519.pub (i.g. cat /opt/app-root/src/.ssh/id_rsa.pub)\n</code></pre></p>"},{"location":"mlops/tasks/task1/#4-add-the-ssh-key-to-your-github-account","title":"4. Add the SSH Key to your GitHub Account","text":"<ul> <li>Go to GitHub &gt; Settings &gt; SSH and GPG keys.</li> <li>Click \"New SSH key\".</li> <li>Paste your copied key and give it a descriptive title.</li> </ul>"},{"location":"mlops/tasks/task1/#5-clone-your-fork-using-ssh-url-and-stored-ssh-key","title":"5. Clone your Fork Using SSH-URL and Stored SSH-Key","text":"<p>Clone the repository you forked in the first step. <pre><code>git clone git@github.com:your-username/your-forked-repo.git\n</code></pre></p>"},{"location":"mlops/tasks/task1/#5-download-the-dataset-into-the-notebook","title":"5. Download the Dataset into the Notebook","text":"<p>The Data for bike sharing company can be found under this link. </p> <pre><code>DATASET_URL: https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip\n</code></pre> <p>Please copy and paste this link into the appropriate section of the first Jupyter Notebook (<code>\"01_data_exploration.ipynb\"</code>).</p> <p>Next, follow the Markdown instructions and execute each code cell to explore, clean, and preprocess the dataset. The final cleaned dataset will be saved in the <code>data/processed</code> directory.</p>"},{"location":"mlops/tasks/task2/","title":"Data Preparations for Model Training","text":"<p>In this task, we take the cleaned dataset from the last task and get it ready for model training. For that, please open the second notebook, <code>\"02_model_training.ipynb\"</code>, and follow the instructions below to complete this task.</p>"},{"location":"mlops/tasks/task2/#1-load-the-processed-data","title":"1. Load the Processed Data","text":"<p>Add the cleaned data for the first two month in the appropriate cell: <pre><code>data_2011_01.csv\ndata_2011_02.csv\n</code></pre></p>"},{"location":"mlops/tasks/task2/#2-set-the-categorical-features","title":"2. Set the Categorical Features","text":"<p>We already have set the numerical features. </p> <p>You only need to set these categorical features: <pre><code>['season', 'holiday', 'workingday', 'weathersit']\n</code></pre></p>"},{"location":"mlops/tasks/task2/#3-split-the-dataset-into-training-and-test-datasets","title":"3. Split the DataSet into Training and Test DataSets","text":"<p>Use <code>train_test_split</code> function from <code>sklearn.model_selection</code> module.</p> <p>Just add this line of code in the appropriate cell, without any changes: <pre><code>train_test_split(X_input, y_input, test_size=0.3, random_state=42)\n</code></pre></p>"},{"location":"mlops/tasks/task2/#4-inspect-the-split-data","title":"4. Inspect the Split Data","text":"<p>Check the shape of your training and test sets to confirm the split:</p> <pre><code>print(X_train.shape, X_test.shape)\n</code></pre> <p>(Optional) You can also print out the first 5 rows of both training and test data:</p> <pre><code>print(X_train.head(), X_test.head()))\n</code></pre> <p>Next, we use these <code>Train</code> and <code>Test</code> datasets to train a machine learning model.</p>"},{"location":"mlops/tasks/task3/","title":"Model Training &amp; Experiment Tracking","text":"<p>In this task, you'll train a machine learning model using the prepared <code>Train</code> and <code>Test</code> datasets in the last task. You'll also use an experiment tracking tool (e.g., MLflow) to log and compare key metrics such as accuracy (e.g. rmse and r2), parameters (e.g. n_estimators and max_depth), and model artifacts. This helps you keep track of different training runs and make informed decisions based on their performance.</p> <p>The steps in this task will be caried out in the second notebook: <code>\"02_model_training.ipynb\"</code></p> <p>\ud83d\udca1 Note: Usually, the appropriate model is selected by data scientists based on the specific problem and characteristics of the data. For the bike sharing forecasting problem, we will use the <code>\"RandomForestRegressor\"</code> model and <code>rmse</code> and <code>r2</code> as deciding metrices.</p>"},{"location":"mlops/tasks/task3/#1-set-the-mlflow-remote-tracking-server","title":"1. Set the MLflow Remote Tracking Server","text":"<p>\ud83d\udca1 Note: The link to the MLflow server will be provided during the workshop!</p> <p>You should replace the <code>MLFLOW_REMOTE_TRACKING_SERVER</code> with this provided URL.</p>"},{"location":"mlops/tasks/task3/#2-set-a-dummy-name-or-your-firstname-it-should-be-unique","title":"2. Set a Dummy Name or your Firstname (It should be unique!)","text":"<p>\ud83d\udca1 Note: There is only one instance of <code>MLFlow server</code> for all the participants. So in order to avoid any confusions, please make sure that you put an unique name!</p> <p>You should replace the <code>YOUR_FIRSTNAME</code> with a dummy name or your firstname.</p>"},{"location":"mlops/tasks/task3/#3-select-model-parameters","title":"3. Select Model Parameters","text":"<p>Choose a set of the model parameters (e.g., max_depth, n_estimators) from a predefined range:</p> <p><code>n_estimators</code>:  Number of decision trees that the model builds (i.e 50, 100, 200)</p> <p><code>max_depth</code>: Maximum depth of each tree (i.e 2, 6, 10, 15)</p> <p><code>random_state</code>: Ensures reproducibility of results by fixing the random seed. (It is set to 42. You should not change it!)</p> <p>This allows you to experiment with different configurations.</p>"},{"location":"mlops/tasks/task3/#4-mlflow-ui-compare-runs","title":"4. MLflow UI - Compare Runs","text":"<p>Go to your Experiment on <code>MLflow UI</code> to compare runs and evaluate model performance based on metrics and parameter choices.</p> <p>\ud83d\udca1 Note: Other participants are also storing their experiemnts on the same instance. So please make sure that you are in the correct experiment.</p> <p>We will see in the next step how to select the best model and register that model on the same <code>MLflow</code> server it.</p>"},{"location":"mlops/tasks/task4/","title":"Review the Experiments &amp; Select the Best Model","text":"<p>In this task, you'll review your logged experiments on <code>MLflow</code> to compare model runs based on their performance metrics. By analyzing the results, you'll identify the best-performing model configuration and select it for further evaluation or deployment.</p> <p>The steps in this task will be caried out in the second notebook: <code>\"02_model_training.ipynb\"</code></p>"},{"location":"mlops/tasks/task4/#1-set-the-mlflow-remote-tracking-server","title":"1. Set the MLflow Remote Tracking Server","text":"<p>\ud83d\udca1 Note: The link to the MLflow server will be provided during the workshop!</p> <p>You should replace the <code>MLFLOW_REMOTE_TRACKING_SERVER</code> with this provided URL.</p>"},{"location":"mlops/tasks/task4/#2-set-a-dummy-name-or-your-firstname-it-should-be-unique","title":"2. Set a Dummy Name or your Firstname (It should be unique!)","text":"<p>\ud83d\udca1 Note: There is only one instance of <code>MLFlow server</code> for all the participants. So in order to avoid any confusions, please make sure that you put an unique name!</p> <p>You should replace the <code>YOUR_FIRSTNAME</code> with a dummy name or your firstname.</p>"},{"location":"mlops/tasks/task4/#3-review-the-experiments-and-their-results","title":"3. Review the Experiments and their Results","text":"<p>At this step, you should just run the cell and review the output.  This is basically, what we saw on the MLflow UI at the end of the last step.</p>"},{"location":"mlops/tasks/task4/#4-select-the-best-performing-experiment","title":"4. Select the Best-Performing experiment","text":"<p>We select the run (<code>run-ID</code>) which has performed the best, when we prompted in the notebook.</p> <p>Please folow the instruction in the notebook to complete this task.</p>"},{"location":"mlops/tasks/task5/","title":"Model Deploymet - Containerize the Endpoint-API","text":"<p>In this task, you will package your trained model into a RESTful API using a web framework (e.g., Flask or FastAPI) and containerize the service (e.g. with Docker, Podman, BuildConfig on OpenShift). This makes the model accessible for real-time predictions via HTTP/S requests, enabling easy deployment to cloud or on-prem environments.</p> <p>The steps in this task will be caried out in the third notebook: <code>\"03_model_deployment.ipynb\"</code></p>"},{"location":"mlops/tasks/task5/#1-review-the-rest-api-app-and-containerfile","title":"1. Review the REST API App and Containerfile","text":"<ul> <li>Inspect the <code>app.py</code> or main API script to understand the structure of the REST endpoint.</li> <li>Review the <code>Containerfile</code> (Dockerfile) to see how the application and model are packaged.</li> <li>Take a note of the variables that should be set by deployment.</li> </ul>"},{"location":"mlops/tasks/task5/#2-go-the-to-the-openshift-console-administrator-perspektive","title":"2. Go the to the OpenShift Console (Administrator Perspektive)","text":"<p>When logged in, go to the <code>Administrator</code> perspektive. You can find it by clicking on <code>Developer</code> at the top left corner. </p> <p>Find your porject (e.g. <code>user1</code>) in the openshift console and click on it, so that the resources in this namespace will be shown. </p>"},{"location":"mlops/tasks/task5/#3-create-an-imagestream-on-openshift","title":"3. Create an ImageStream on OpenShift","text":"<p>Go to <code>Buils -&gt; ImageStream</code> to create an ImageStream on OpenShift to track and manage the container image built for your model API, allowing seamless integration with BuildConfig and deployments.</p> <pre><code>kind: ImageStream\napiVersion: image.openshift.io/v1\nmetadata:\n  name: bike-sharing-imagestream\n</code></pre>"},{"location":"mlops/tasks/task5/#4-create-a-buildconfig-on-openshift","title":"4. Create a BuildConfig on OpenShift","text":"<p>You can find this resource under <code>Buils -&gt; BuildConfig</code>.</p> <p>Click on create builconfig and go to the <code>yaml</code> view and paste these line there. <pre><code>apiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: bike-sharing-buildconfig\nspec:\n  source:\n    type: Git\n    git:\n      uri: 'https://github.com/FORK_REPO_PARTICIPATS/MLOps-Workshop-Exercises.git'\n    contextDir: workshop_materials/bike_demand_forecasting/models\n  strategy:\n    type: Docker\n    dockerStrategy:\n      dockerfilePath: Containerfile\n      from:\n        kind: DockerImage\n        name: 'python:3.11-slim'\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'bike-sharing-imagestream:1.0'\n</code></pre></p> <p>This config tells OpenShift how to build the container image from your source code.</p> <p>Now click on Action on top right corner and select Start Build. </p> <p>You can see the process of building the image in the respective builds-run under the tab <code>Builds</code>. </p> <p>When the <code>Status</code> is <code>Complete</code>, it means that the image is created and stored in the <code>ImageStream</code>.</p> <p>Now if you go to the ImageStream (<code>bike-sharing-imagestream</code>) you created in the last step, the built image is to be seen under <code>Tags</code>.</p> <p>In the next task, we take this image and deploy it on the OpenShift Cluster. </p>"},{"location":"mlops/tasks/task6/","title":"Model Deploymet - Deploy on OpenShift Cluster","text":"<p>In this task, you'll deploy your containerized model API to an OpenShift cluster. This involves creating the necessary Kubernetes resources (such as deployments and services), exposing the API endpoint, and ensuring your model is accessible and scalable in a production-like environment.</p> <p>We will test the deployment by sending a batch inferencing request using the Test Dataset to verify the model's functionality.</p> <p>The steps in this task will be caried out in the third notebook: <code>03_model_deployment.ipynb</code></p>"},{"location":"mlops/tasks/task6/#1-go-the-to-the-openshift-console","title":"1. Go the to the OpenShift Console","text":"<p>Find your porject (e.g. <code>user1</code>) in the openshift console.</p>"},{"location":"mlops/tasks/task6/#2-create-a-deployment-using-the-built-image","title":"2. Create a Deployment Using the Built Image","text":"<p>From the left pannel go to <code>Workloads -&gt; Deployment</code> and create a new deployment. </p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bike-model-api\n  labels:\n    app: bike-model-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bike-model-api\n  template:\n    metadata:\n      labels:\n        app: bike-model-api\n    spec:\n      containers:\n        - name: bike-model-container\n          image: IMAGE_URL_FROM_IMAGE_STREAM  # It can be found on the imagestream page in OpenShift\n          ports:\n            - containerPort: 8000\n          env:\n            - name: MLFLOW_TRACKING_URI\n              value: \"MLFLOW_REMOTE_TRACKING_SERVER\"\n            - name: MODEL_NAME\n              value: \"MODEL_NAME\" # f\"BikeSharingModel_{PARTICIPANT_FIRSTNAME}\"\n            - name: MODEL_VERSION\n              value: \"1\"  # Or leave it empty for latest Production version\n          resources:\n            requests:\n              cpu: \"250m\"\n              memory: \"256Mi\"\n            limits:\n              cpu: \"500m\"\n              memory: \"512Mi\"\n</code></pre> <p>\ud83d\udca1 Note One: Please set the link to the image from the last task. It could also be found in the created <code>ImageStream</code>.</p> <p>\ud83d\udca1 Note Two: Make sure that you set the currect values for the environment variables (e.g. <code>MLFLOW_TRACKING_URI</code>, <code>MODEL_NAME</code>, <code>MODEL_VERSION</code>) and replace the placeholders.</p> <p><code>MODEL_VERSION</code> is already set to <code>1</code>, but if you have more that 1 version and you want to deploy that, please change the value for variable <code>MODEL_VERSION</code> accordingly.</p> <p><code>MLFLOW_TRACKING_URI</code> is the same server you have set in step 3 &amp; 4.</p>"},{"location":"mlops/tasks/task6/#3-service-expose-the-api-endpoint-internally","title":"3. Service: Expose the API Endpoint internally","text":"<p>Create a service to expose your API internally for applications on the same cluster:</p> <p>From the left pannel go to <code>Network -&gt; Service</code> and create a new service.  <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: bike-model-api-svc\nspec:\n  selector:\n    app: bike-model-api\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8000\n  type: ClusterIP\n</code></pre></p> <p>Now, the model is reachable only from inside the cluster!</p>"},{"location":"mlops/tasks/task6/#4-route-optional-expose-the-api-endpoint-externally","title":"4. Route (Optional): Expose the API Endpoint externally","text":"<p>Deploy this resource ONLY and ONLY if you want to make your model accessible outside the OpenShift cluster. From the left pannel go to <code>Network -&gt; Route</code> and create a new route. <pre><code>apiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: bike-model-api-route\nspec:\n  port:\n    targetPort: 8000 \n  to:\n    kind: Service\n    name: bike-model-api-svc\n  tls:\n    termination: edge  # or passthrough, depending on your needs\n    insecureEdgeTerminationPolicy: Redirect  # Redirect HTTP to HTTPS\n</code></pre></p>"},{"location":"mlops/tasks/task6/#5-prepare-the-test-data","title":"5. Prepare the Test Data","text":"<p>A sample test dataset (e.g., a few rows of processed features) is already created for you in CSV or JSON format that matches the model\u2019s input schema: <code>workshop_materials/bike_demand_forecasting/data/test_model</code>.</p> <p>Run the cells in the notebook to take this test dataset and create the inference request.</p>"},{"location":"mlops/tasks/task6/#6-set-the-model-deployment-endpoint","title":"6. Set the Model Deployment Endpoint","text":"<p>In the notebook, the <code>MODEL_API_SERVICE</code> should be replaced with the service url we created in step 3. This url can be found, when going to the <code>Networking -&gt; Service -&gt; bike-model-api-svc</code>. It is shown under the <code>Hostname</code> and end with <code>svc.cluster.local</code>.</p>"},{"location":"mlops/tasks/task6/#7-simple-and-batch-inferencing","title":"7. Simple and Batch Inferencing","text":"<p>As you follow the instruction in the notebook to send requests to test the model endpoint, you see that the prediction is returned for all the test data stored in the sample dataset.</p> <p>Finally, a script is provided to visualize the <code>Actual vs Predicted Counts</code> for sample inputs.</p>"},{"location":"mlops/tasks/task7/","title":"Model &amp; Data Monitoring","text":"<p>In this task, you'll implement basic monitoring to track your model\u2019s performance and input data quality over time. This includes observing metrics such as prediction accuracy, data drift, and request volume to ensure the deployed model remains reliable and continues to perform well in a real-world environment.</p> <p>Here we will be working in the fourth notebook: <code>04_model_monitoring.ipynb</code></p> <p>Since many of the cells automatically load the reference and batch data, fit the Evidently reports, and visualize results directly in your Jupyter environment, your only tasks are to <code>set the model endpoint</code> and <code>run the notebook</code> from top to bottom without modification or additional assignments. As you execute, observe how Evidently surfaces insights like data drift, model performance over time, and input data quality\u2014all rendered interactively. This end-to-end execution gives you hands-on exposure to model and data monitoring workflows using Evidently in a practical forecasting scenario.</p>"},{"location":"mlops/tasks/task7/#set-the-model-deployment-endpoint","title":"Set the Model Deployment Endpoint","text":"<p>In the notebook, the <code>MODEL_API_SERVICE</code> should be replaced with the service url we created in step 2. This url can be found, when going to the <code>Networking -&gt; Service -&gt; bike-model-api-svc</code>. It is shown under the <code>Hostname</code> and end with <code>svc.cluster.local</code>.</p>"},{"location":"mlops/tasks/task8_kubeflow/","title":"Automating the Workflow with Kubeflow Pipelines","text":""},{"location":"mlops/tasks/task8_kubeflow/#kubeflow-pipeline-reading-data-model-training-tracking-registering","title":"(Kubeflow Pipeline - Reading Data &amp; Model Training, Tracking &amp; Registering)","text":"<p>Using Kubeflow Pipelines, you'll automate the end-to-end workflow for reading data, training a model, tracking experiments, and registering the trained model. This ensures your ML process is reproducible and scalable, with automatic logging of parameters, metrics, and artifacts.</p> <p>The steps in this task will be caried out in the directory <code>\"workshop_materials/bike_demand_forecasting_pipeline\"</code>.</p>"},{"location":"mlops/tasks/task8_kubeflow/#1-set-the-variables-for-the-pipeline","title":"1. Set the Variables for the Pipeline","text":"<p>Update the python file (<code>pipeline_bike_sharing.py</code>) containing pipeline logic accordingly:  </p> <ul> <li> <p>The image name for the pipeline (<code>IMAGE_FOR_PIPELINE</code>):     <pre><code>quay.io/modh/runtime-images:runtime-datascience-ubi9-python-3.11-20250703\n</code></pre></p> </li> <li> <p>The link to the Dataset (<code>DATASET_URL</code>):     <pre><code>https://archive.ics.uci.edu/static/public/275/bike+sharing+dataset.zip\n</code></pre></p> </li> <li> <p>Set a dummy name or your firstname (it should be unique!) \ud83d\udca1 Note One: Letters should be all in lowercase  \ud83d\udca1 Note Two: There is only one instance of <code>MLFlow server</code> for all the participants. So in order to avoid any confusions, please make sure that you put an unique name!</p> </li> </ul> <p>You should replace the <code>YOUR_FIRSTNAME</code> with a dummy name or your firstname.</p>"},{"location":"mlops/tasks/task8_kubeflow/#2-convert-the-pipeline-from-python-to-yaml-format","title":"2. Convert the Pipeline from python to yaml format","text":"<p>RUN the pipeline</p> <p>Look up the MLflow GUI and see the second registered model</p> <p>Edit the deployment to load the second registered model</p> <p>Just see the logs for the pod with which the new model is deployed.</p>"},{"location":"mlops/tasks/task9_elyra/","title":"Task9 elyra","text":""},{"location":"mlops/tasks/task9_elyra/#automating-the-workflow-with-elyra-pipelines","title":"\ud83d\udd04 Automating the Workflow with Elyra Pipelines","text":"<p>After completing the manual execution of each notebook, the next step is to automate the workflow using Elyra's pipeline capabilities. Elyra allows you to visually compose, configure, and execute pipelines directly within JupyterLab, streamlining the machine learning lifecycle.</p>"},{"location":"mlops/tasks/task9_elyra/#instructions","title":"\ud83d\udee0\ufe0f Instructions:","text":""},{"location":"mlops/tasks/task9_elyra/#1-open-elyra-pipeline-editor","title":"1. Open Elyra Pipeline Editor:","text":"<ul> <li>In JupyterLab, click on the Launcher tab.</li> <li>Under the Elyra section, select Pipeline Editor to create a new pipeline.</li> </ul>"},{"location":"mlops/tasks/task9_elyra/#2-configure-the-pipeline-environment","title":"2. Configure the Pipeline Environment:","text":"<p>Before adding notebooks, ensure your Elyra environment is properly configured:</p> <ul> <li>Connect to S3 Storage: Ensure you have access to an S3-compatible object store (e.g., AWS S3, MinIO).</li> <li> <p>Define Kubernetes Secrets: Use an existing Kubernetes secret that includes the following keys:</p> <ul> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> <li><code>AWS_S3_BUCKET</code></li> <li><code>AWS_S3_ENDPOINT</code></li> </ul> </li> <li> <p>These secrets should be referenced in your runtime configuration to allow pipeline nodes to read/write from S3, as shown in these images:</p> <p> </p> </li> <li> <p>Set a Default Runtime Configuration:</p> <ul> <li>Go to Elyra\u2019s Pipeline Settings.</li> <li>Select or create a Runtime Configuration pointing to your Kubernetes-based execution environment (e.g., Kubeflow Pipelines, Apache Airflow, or local).</li> <li>Assign this configuration as default to streamline pipeline runs.</li> </ul> </li> </ul>"},{"location":"mlops/tasks/task9_elyra/#3-add-notebooks-to-the-pipeline","title":"3. Add Notebooks to the Pipeline:","text":"<ul> <li>From the file browser, drag and drop the following notebooks onto the pipeline canvas:<ul> <li><code>01_data_exploration.ipynb</code></li> <li><code>02_model_training.ipynb</code></li> <li><code>03_model_deployment.ipynb</code></li> <li><code>04_drift_reports.ipynb</code></li> </ul> </li> </ul>"},{"location":"mlops/tasks/task9_elyra/#4-define-execution-order","title":"4. Define Execution Order:","text":"<pre><code>- Connect the notebooks in the order listed above by drawing lines between them, establishing the execution sequence.\n</code></pre>"},{"location":"mlops/tasks/task9_elyra/#5-configure-node-properties","title":"5. Configure Node Properties:","text":"<ul> <li>For each notebook node, specify the following:<ul> <li>Runtime Image: Select an appropriate Docker image that contains the necessary dependencies.</li> <li>File Dependencies: List any files required by the notebook.</li> <li>Output Files: Specify the files generated by the notebook that will be used in subsequent steps.</li> <li>Environment Variables: Set any environment variables needed for execution.</li> </ul> </li> </ul>"},{"location":"mlops/tasks/task9_elyra/#6-save-the-pipeline","title":"6. Save the Pipeline:","text":"<ul> <li>Click on File &gt; Save Pipeline and name your pipeline, for example, <code>bike_demand_forecasting.pipeline</code>.</li> </ul>"},{"location":"mlops/tasks/task9_elyra/#7-run-the-pipeline","title":"7. Run the Pipeline:","text":"<ul> <li>Click on the Run Pipeline button (\u25b6\ufe0f) in the pipeline editor toolbar.</li> <li>In the run configuration dialog:<ul> <li>Pipeline Name: Enter a name for this run instance.</li> <li>Runtime Configuration: Choose the configuration you prepared in step 2.</li> </ul> </li> <li>Click Run to execute the pipeline.</li> </ul>"},{"location":"mlops/tasks/task9_elyra/#8-monitor-execution","title":"8. Monitor Execution:","text":"<ul> <li>Observe the execution progress in the Pipeline Editor and the JupyterLab console.</li> <li>Upon completion, verify the outputs in the designated directories (e.g., data/processed/, models/, reports/), including any artifacts written to S3.</li> </ul> <p>By automating the workflow with Elyra, you ensure consistency, reproducibility, and efficiency in your machine learning processes. This structured automation prepares the ground for continuous integration and deployment in real-world MLOps systems.</p>"}]}