# Review the Experiments & Select the Best Model
In this task, you'll review your logged MLflow experiments to compare model runs based on their performance metrics. By analyzing the results, you'll identify the best-performing model configuration and select it for further evaluation or deployment.

The steps in this task will be caried out in the second notebook: `02_model_training.ipynb`

### 1. Set the MLflow Remote Tracking Server
**The link to the MLflow server will be provided during the workshop!**

You should replace the `MLFLOW_REMOTE_TRACKING_SERVER` with this provided URL.

### 2. Review the Experiments
At this step, you should just run the cell and review the output. 
This is basically, what we saw on the MLflow UI at the end of the last step.

### 3. Select the Best-Performing experiment
We select the run (``run-ID``) which has performed the best, when we prompted in the notebook.

Please folow the instruction in the notebook to complete this task.